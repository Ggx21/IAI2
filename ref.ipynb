{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path=\"testset\"\n",
    "train_path=\"testset\\\\train.txt\"\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv(train_path,names=[\"label\",\"comment\"],sep=\"\\t\")\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_len=train_data.iloc[:,1].apply(lambda x:len(x))\n",
    "#每一条评论的词数个数。\n",
    "comments_len\n",
    "\n",
    "train_data[\"comments_len\"]=comments_len\n",
    "train_data[\"comments_len\"].describe(percentiles=[.5,.95])#本来是三分位的，这里自己选了两个"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "words=[]\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    com=train_data[\"comment\"][i].split()\n",
    "    words=words+com\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我只要那些出现频率大于10的词，这个你可以自己调节，我这里由于计算资源不太雄厚，所以10比较高。不过，其实那些“难看”，“好看”这样的词按理在19998条数据中应该是绝对超过30的，所以我也算是保留了情感分类的关键词。\n",
    "\n",
    "这个时候，我们的文件夹下多了一个文件:\"word_freq.txt\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq=10\n",
    "import os\n",
    "with open(os.path.join(root_path,\"word_freq.txt\"), 'w', encoding='utf-8') as fout:\n",
    "    for word,freq in Counter(words).most_common():\n",
    "        if freq>Freq:\n",
    "            fout.write(word+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化vocab\n",
    "with open(os.path.join(root_path,\"word_freq.txt\"), encoding='utf-8') as fin:\n",
    "    vocab = [i.strip() for i in fin]\n",
    "vocab=set(vocab)\n",
    "word2idx = {i:index for index, i in enumerate(vocab)}\n",
    "idx2word = {index:i for index, i in enumerate(vocab)}#没有想到列表竟然可以枚举。\n",
    "vocab_size = len(vocab)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id=word2idx[\"把\"]\n",
    "print(pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 62\n",
    "#对输入数据进行预处理,主要是对句子用索引表示且对句子进行截断与padding，将填充使用”把“来。\n",
    "def tokenizer():\n",
    "    inputs = []\n",
    "    sentence_char = [i.split() for i in train_data[\"comment\"]]\n",
    "    # 将输入文本进行padding\n",
    "    for index,i in enumerate(sentence_char):\n",
    "        temp=[word2idx.get(j,pad_id) for j in i]#表示如果词表中没有这个稀有词，无法获得，那么就默认返回pad_id。\n",
    "        if(len(i)<sequence_length):\n",
    "            #应该padding。\n",
    "            for _ in range(sequence_length-len(i)):\n",
    "                temp.append(pad_id)\n",
    "        else:\n",
    "            temp = temp[:sequence_length]\n",
    "        inputs.append(temp)\n",
    "    return inputs\n",
    "data_input = tokenizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备训练和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\"\n",
    "Embedding_size = 50\n",
    "Batch_Size = 32\n",
    "Kernel = 3\n",
    "Filter_num = 10#卷积核的数量。\n",
    "Epoch = 60\n",
    "Dropout = 0.5\n",
    "Learning_rate = 1e-3\n",
    "\n",
    "class TextCNNDataSet(Data.Dataset):\n",
    "    def __init__(self, data_inputs, data_targets):\n",
    "        self.inputs = torch.LongTensor(data_inputs)\n",
    "        self.label = torch.LongTensor(data_targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.label[index] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "TextCNNDataSet = TextCNNDataSet(data_input, list(train_data[\"label\"]))\n",
    "train_size = int(len(data_input) * 0.2)\n",
    "test_size = int(len(data_input) * 0.05)\n",
    "val_size= len(data_input) -train_size-test_size#乘以0.75反而报错，因为那个有取整，所以导致了舍入。\n",
    "train_dataset,val_dataset,test_dataset = torch.utils.data.random_split(TextCNNDataSet, [train_size,val_size, test_size])\n",
    "\n",
    "TrainDataLoader = Data.DataLoader(train_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "TestDataLoader = Data.DataLoader(test_dataset, batch_size=Batch_Size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载词向量模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import keyedvectors\n",
    "w2v=keyedvectors.load_word2vec_format(os.path.join(root_path,\"wiki_word2vec_50.bin\"),binary=True)\n",
    "\n",
    "w2v[[\"的\",\"在\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_l=list(vocab)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(vocab_l))):\n",
    "    try:\n",
    "        w2v[vocab_l[i]]=w2v[vocab[i]]\n",
    "    except Exception as e:\n",
    "        w2v[vocab_l[i]]=np.random.randn(50,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18344\\AppData\\Local\\Temp\\ipykernel_4636\\2308185316.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, 训练准确率=0.49986290311813353\n",
      "epoch = 2, 训练准确率=0.5148306450843811\n",
      "epoch = 3, 训练准确率=0.5473870968818665\n",
      "epoch = 4, 训练准确率=0.5494032258987427\n",
      "epoch = 5, 训练准确率=0.5669032258987426\n",
      "epoch = 6, 训练准确率=0.560895161151886\n",
      "epoch = 7, 训练准确率=0.5698629031181336\n",
      "epoch = 8, 训练准确率=0.5841209676265716\n",
      "epoch = 9, 训练准确率=0.587895161151886\n",
      "epoch = 10, 训练准确率=0.598645161151886\n",
      "epoch = 11, 训练准确率=0.592145161151886\n",
      "epoch = 12, 训练准确率=0.6071532258987427\n",
      "epoch = 13, 训练准确率=0.6044032258987426\n",
      "epoch = 14, 训练准确率=0.6126451611518859\n",
      "epoch = 15, 训练准确率=0.6164112901687622\n",
      "epoch = 16, 训练准确率=0.6228709676265717\n",
      "epoch = 17, 训练准确率=0.6319032258987427\n",
      "epoch = 18, 训练准确率=0.6524193549156189\n",
      "epoch = 19, 训练准确率=0.6314032258987426\n",
      "epoch = 20, 训练准确率=0.6449032258987427\n",
      "epoch = 21, 训练准确率=0.6474193549156189\n",
      "epoch = 22, 训练准确率=0.6612016129493713\n",
      "epoch = 23, 训练准确率=0.6541854839324951\n",
      "epoch = 24, 训练准确率=0.667895161151886\n",
      "epoch = 25, 训练准确率=0.6766774191856384\n",
      "epoch = 26, 训练准确率=0.6661532258987427\n",
      "epoch = 27, 训练准确率=0.6664032258987427\n",
      "epoch = 28, 训练准确率=0.6789193549156189\n",
      "epoch = 29, 训练准确率=0.6799112901687622\n",
      "epoch = 30, 训练准确率=0.6961854839324951\n",
      "epoch = 31, 训练准确率=0.6796612901687622\n",
      "epoch = 32, 训练准确率=0.687145161151886\n",
      "epoch = 33, 训练准确率=0.7046532258987427\n",
      "epoch = 34, 训练准确率=0.6904435482025146\n",
      "epoch = 35, 训练准确率=0.7036693549156189\n",
      "epoch = 36, 训练准确率=0.7006774191856384\n",
      "epoch = 37, 训练准确率=0.6976854839324951\n",
      "epoch = 38, 训练准确率=0.6941532258987426\n",
      "epoch = 39, 训练准确率=0.695145161151886\n",
      "epoch = 40, 训练准确率=0.6944354839324951\n",
      "epoch = 41, 训练准确率=0.7146612901687622\n",
      "epoch = 42, 训练准确率=0.7059354839324952\n",
      "epoch = 43, 训练准确率=0.7092096772193909\n",
      "epoch = 44, 训练准确率=0.6999193549156189\n",
      "epoch = 45, 训练准确率=0.7199193549156189\n",
      "epoch = 46, 训练准确率=0.718645161151886\n",
      "epoch = 47, 训练准确率=0.7094193549156189\n",
      "epoch = 48, 训练准确率=0.7132016129493713\n",
      "epoch = 49, 训练准确率=0.7149354839324951\n",
      "epoch = 50, 训练准确率=0.7106693549156189\n",
      "epoch = 51, 训练准确率=0.7246290321350097\n",
      "epoch = 52, 训练准确率=0.7026532258987427\n",
      "epoch = 53, 训练准确率=0.7071774191856385\n",
      "epoch = 54, 训练准确率=0.7219274191856384\n",
      "epoch = 55, 训练准确率=0.728895161151886\n",
      "epoch = 56, 训练准确率=0.7274354839324951\n",
      "epoch = 57, 训练准确率=0.7136532258987427\n",
      "epoch = 58, 训练准确率=0.7154435482025147\n",
      "epoch = 59, 训练准确率=0.7156532258987427\n",
      "epoch = 60, 训练准确率=0.7321612901687622\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "def word2vec(x):\n",
    "    #x:batch_size,sequence_length\n",
    "    #-》x:batch_size,sequence_length,embedding_size\n",
    "    #x是以编号的形式来反映的，所以需要将其翻译一下。\n",
    "    x2v=np.ones((len(x),x.shape[1],Embedding_size))\n",
    "    for i in range(len(x)):\n",
    "#         seqtext=[idx2char[j.item()] for j in x[i]]\n",
    "        x2v[i]=w2v[[idx2word[j.item()] for j in x[i]]]\n",
    "    return torch.tensor(x2v).to(torch.float32)\n",
    "\n",
    "#使用word2vec版本的。\n",
    "num_classs = 2#2分类问题。\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "#         self.W = nn.Embedding(vocab_size, embedding_dim=Embedding_size)\n",
    "        out_channel = Filter_num #可以等价为通道的解释。\n",
    "        self.conv = nn.Sequential(\n",
    "                    nn.Conv2d(1, out_channel, (2, Embedding_size)),#卷积核大小为2*Embedding_size,默认当然是步长为1\n",
    "                    nn.ReLU(),\n",
    "                    nn.MaxPool2d((sequence_length-1,1)),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(Dropout)\n",
    "        self.fc = nn.Linear(out_channel, num_classs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        #x:batch_size*seq_len\n",
    "        embedding_X =  word2vec(X)\n",
    "        # batch_size, sequence_length, embedding_size\n",
    "        embedding_X = embedding_X.unsqueeze(1)\n",
    "        # batch_size, 1,sequence_length, embedding_size\n",
    "        conved = self.conv(embedding_X)\n",
    "        #batch_size,10,seq_len-1,1\n",
    "        #batch_size,10,seq_len-1,1\n",
    "        #batch_size,10,1,1########直接被maxpooliing了，从一个序列变成一个向量，表示将整个句子选出一个最关键的情感分类词来。\n",
    "        conved = self.dropout(conved)\n",
    "        flatten = conved.view(batch_size, -1)\n",
    "        # [batch_size, 10]\n",
    "        output = self.fc(flatten)\n",
    "        #2分类问题，往往使用softmax，表示概率。\n",
    "        return F.log_softmax(output)\n",
    "    \n",
    "model = TextCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=Learning_rate)\n",
    "\n",
    "def binary_acc(pred, y):\n",
    "    \"\"\"\n",
    "    计算模型的准确率\n",
    "    :param pred: 预测值\n",
    "    :param y: 实际真实值\n",
    "    :return: 返回准确率\n",
    "    \"\"\"\n",
    "    correct = torch.eq(pred, y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc.item()\n",
    "\n",
    "def train():\n",
    "    avg_acc = []\n",
    "    model.train()\n",
    "    for index, (batch_x, batch_y) in enumerate(TrainDataLoader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        pred = model(batch_x)\n",
    "        loss = F.nll_loss(pred, batch_y)\n",
    "        acc = binary_acc(torch.max(pred, dim=1)[1], batch_y)\n",
    "        avg_acc.append(acc)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_acc\n",
    "\n",
    "# Training cycle\n",
    "model_train_acc, model_test_acc = [], []\n",
    "for epoch in range(Epoch):\n",
    "    train_acc = train()\n",
    "    print(\"epoch = {}, 训练准确率={}\".format(epoch + 1, train_acc))\n",
    "    model_train_acc.append(train_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网格"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \"\"\"\n",
    "    模型评估\n",
    "    :param model: 使用的模型\n",
    "    :return: 返回当前训练的模型在测试集上的结果\n",
    "    \"\"\"\n",
    "    avg_acc = []\n",
    "    model.eval()  # 进入测试模式\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in TestDataLoader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            pred = model(x_batch)\n",
    "            acc = binary_acc(torch.max(pred, dim=1)[1], y_batch)\n",
    "            avg_acc.append(acc)\n",
    "    return np.array(avg_acc).mean()\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_train_acc)\n",
    "plt.ylim(ymin=0.5, ymax=0.8)\n",
    "plt.title(\"The accuracy of textCNN model\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
